{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from os import walk\n",
    "\n",
    "import difflib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_list = []\n",
    "\n",
    "for dirpath, dirnames, filenames in walk('../repositories/raw_pages'):\n",
    "    if '.ipynb_checkpoints' in dirpath:\n",
    "        continue\n",
    "    \n",
    "    dirpath = dirpath.replace('\\\\', '/') # для Windows\n",
    "    for fn in filenames:\n",
    "        if '.DS_Store' in fn:\n",
    "            continue \n",
    "            \n",
    "        fp = f'{dirpath}/{fn}'\n",
    "        pages_list.append(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_texts = {}\n",
    "\n",
    "for dirpath, dirnames, filenames in walk(page_texts_path):\n",
    "    if '.ipynb_checkpoints' in dirpath:\n",
    "        continue\n",
    "        \n",
    "    for fn in filenames:\n",
    "        if '.DS_Store' in fn:\n",
    "            continue\n",
    "            \n",
    "        fp = f'{dirpath}/{fn}'\n",
    "        with open(fp, 'r', encoding=\"utf-8\") as f:\n",
    "            pt = ' '.join(f.readlines())\n",
    "        \n",
    "        original_link = fp.split('/')[-1].replace('|', '/').replace('.txt', '')\n",
    "        pages_texts[original_link] = pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2.tar.gz (23.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m509.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m340.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.11.4-cp312-cp312-macosx_12_0_arm64.whl.metadata (217 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading numpy-1.26.2-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m260.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.11.4-cp312-cp312-macosx_12_0_arm64.whl (29.6 MB)\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m342.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: gensim\n",
      "  Building wheel for gensim (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-4.3.2-cp312-cp312-macosx_13_0_arm64.whl size=23951620 sha256=e9e84c5395383d34d7a7717fbaf1fa63d6e559011d76676f8c8b2b950a707ba9\n",
      "  Stored in directory: /Users/maksimmigur/Library/Caches/pip/wheels/50/c0/ac/7bb08954bc59d390c848b480a3fc5eec68c14bc77bf334d624\n",
      "Successfully built gensim\n",
      "Installing collected packages: smart-open, numpy, scipy, gensim\n",
      "Successfully installed gensim-4.3.2 numpy-1.26.2 scipy-1.11.4 smart-open-6.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'lemmatize' from 'gensim.utils' (/Users/maksimmigur/Library/Caches/pypoetry/virtualenvs/searchmodel-fbYcB3Uw-py3.12/lib/python3.12/site-packages/gensim/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m lemmatize\n\u001b[1;32m      3\u001b[0m lemmatize(\u001b[39m\"\u001b[39m\u001b[39mприветик\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'lemmatize' from 'gensim.utils' (/Users/maksimmigur/Library/Caches/pypoetry/virtualenvs/searchmodel-fbYcB3Uw-py3.12/lib/python3.12/site-packages/gensim/utils.py)"
     ]
    }
   ],
   "source": [
    "from gensim.utils impor\n",
    "\n",
    "lemmatize(\"приветик\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/ln/l54hjc713kxgxkw6rfxxgyh00000gn/T/ipykernel_75553/262380060.py:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s\\s+', ' ', text)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'inspect' has no attribute 'getargspec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstring\u001b[39;00m \u001b[39mimport\u001b[39;00m punctuation\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(punctuation)\n\u001b[0;32m----> 7\u001b[0m MORPH \u001b[39m=\u001b[39m MorphAnalyzer()\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(text):\n\u001b[1;32m     10\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/searchmodel-fbYcB3Uw-py3.12/lib/python3.12/site-packages/pymorphy2/analyzer.py:224\u001b[0m, in \u001b[0;36mMorphAnalyzer.__init__\u001b[0;34m(self, path, lang, result_type, units, probability_estimator_cls, char_substitutes)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result_type_orig \u001b[39m=\u001b[39m result_type\n\u001b[1;32m    223\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_char_substitutes(char_substitutes)\n\u001b[0;32m--> 224\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_units(units)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/searchmodel-fbYcB3Uw-py3.12/lib/python3.12/site-packages/pymorphy2/analyzer.py:235\u001b[0m, in \u001b[0;36mMorphAnalyzer._init_units\u001b[0;34m(self, units_unbound)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    234\u001b[0m     \u001b[39mfor\u001b[39;00m unit \u001b[39min\u001b[39;00m item[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 235\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_units\u001b[39m.\u001b[39mappend((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_unit(unit), \u001b[39mFalse\u001b[39;00m))\n\u001b[1;32m    236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_units\u001b[39m.\u001b[39mappend((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_unit(item[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), \u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m    237\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/searchmodel-fbYcB3Uw-py3.12/lib/python3.12/site-packages/pymorphy2/analyzer.py:246\u001b[0m, in \u001b[0;36mMorphAnalyzer._bound_unit\u001b[0;34m(self, unit)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_bound_unit\u001b[39m(\u001b[39mself\u001b[39m, unit):\n\u001b[0;32m--> 246\u001b[0m     unit \u001b[39m=\u001b[39m unit\u001b[39m.\u001b[39;49mclone()\n\u001b[1;32m    247\u001b[0m     unit\u001b[39m.\u001b[39minit(\u001b[39mself\u001b[39m)\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m unit\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/searchmodel-fbYcB3Uw-py3.12/lib/python3.12/site-packages/pymorphy2/units/base.py:35\u001b[0m, in \u001b[0;36mBaseAnalyzerUnit.clone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclone\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_params())\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/searchmodel-fbYcB3Uw-py3.12/lib/python3.12/site-packages/pymorphy2/units/base.py:76\u001b[0m, in \u001b[0;36mBaseAnalyzerUnit._get_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     74\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Return a dict with the parameters for this analyzer unit. \"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\n\u001b[0;32m---> 76\u001b[0m         (key, \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, key, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_param_names()\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/searchmodel-fbYcB3Uw-py3.12/lib/python3.12/site-packages/pymorphy2/units/base.py:70\u001b[0m, in \u001b[0;36mBaseAnalyzerUnit._get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m:\n\u001b[1;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m---> 70\u001b[0m args, varargs, kw, default \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mgetargspec(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39m(args[\u001b[39m1\u001b[39m:])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'inspect' has no attribute 'getargspec'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from string import punctuation\n",
    "from gensim.utils import lemmatize\n",
    "\n",
    "print(punctuation)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\s\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_sentences(text):\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    sentences = []\n",
    "    for s in nltk.sent_tokenize(text):\n",
    "        wrds = []\n",
    "        for wrd in nltk.word_tokenize(s):\n",
    "            if wrd in punctuation:\n",
    "                continue\n",
    "            \n",
    "            wrd = MORPH.parse(wrd)[0].normal_form\n",
    "            wrds.append(wrd)\n",
    "            \n",
    "        clear_sentence = ' '.join(wrds)\n",
    "        \n",
    "        sentences.append(clear_sentence)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "searchmodel-fbYcB3Uw-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
